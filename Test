# python3 -m spacy download en_core_web_sm

nlp = spacy.load("en_core_web_sm")

example_category = res[res['label_st1']==11].reset_index(drop=True)
example_category 

example_doc = nlp(list(example_category['text'])[5])
print(f'{example_doc}\n')

for token in example_doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_ , token.is_stop)
    
 res['text'] = res['text'].astype('str')
res['category'] = res['category'].astype('str')

# helper functions for creating labels
def extract_labels(category_docs, print_word_counts=False):
    """
    Extract labels from documents in the same cluster by concatenating
    most common verbs, objects, and nouns

    Argument:
        category_docs: list of documents, all from the same category or
                       clustering
        print_word_counts: bool, True will print word counts of each type in this category

    Returns:
        label: str, group label derived from concatentating most common
               verb, object, and two most common nouns

    """

    verbs = []
    dobjs = []
    nouns = []
    adjs = []
    
    verb = ''
    dobj = ''
    noun1 = ''
    noun2 = ''

    # for each document, append verbs, dobs, nouns, and adjectives to 
    # running lists for whole cluster
    for i in range(len(category_docs)):
        doc = nlp(category_docs[i])
        for token in doc:
            if token.is_stop==False:
                if token.dep_ == 'ROOT':
                    verbs.append(token.text.lower())

                elif token.dep_=='dobj':
                    dobjs.append(token.lemma_.lower())

                elif token.pos_=='NOUN':
                    nouns.append(token.lemma_.lower())
                    
                elif token.pos_=='ADJ':
                    adjs.append(token.lemma_.lower())

    # for printing out for inspection purposes
    if print_word_counts:
        for word_lst in [verbs, dobjs, nouns, adjs]:
            counter=collections.Counter(word_lst)
            print(counter)
    
    # take most common words of each form
    if len(verbs) > 0:
        verb = collections.Counter(verbs).most_common(1)[0][0]
        
    if len(dobjs) > 0:
        dobj = collections.Counter(dobjs).most_common(1)[0][0]
    
    if len(nouns) > 0:
        noun1 = collections.Counter(nouns).most_common(1)[0][0]
    
    if len(set(nouns)) > 1:
        noun2 = collections.Counter(nouns).most_common(2)[0][0]
    
    # concatenate the most common verb-dobj-noun1-noun2 (if they exist)
    label_words = [verb, dobj]
    
    for word in [noun1, noun2]:
        if word not in label_words:
            label_words.append(word)
    
    if '' in label_words:
        label_words.remove('')
    
    label = '_'.join(label_words)
    
    return label


def apply_and_summarize_labels(df, category_col):
    """
    Assign groups to original documents and provide group counts

    Arguments:
        df: pandas dataframe of original documents of interest to
            cluster
        category_col: str, column name corresponding to categories or clusters

    Returns:
        summary_df: pandas dataframe with model cluster assignment, number
                    of documents in each cluster and derived labels
    """
    
    numerical_labels = df[category_col].unique()
    
    # create dictionary of the numerical category to the generated label
    label_dict = {}
    for label in numerical_labels:
        current_category = list(df[df[category_col]==label].reset_index(drop=True)['text'])
        label_dict[label] = extract_labels(current_category)
        
    # create summary dataframe of numerical labels and counts
    summary_df = (df.groupby(category_col)['text'].count()
                    .reset_index()
                    .rename(columns={'text':'count'})
                    .sort_values('count', ascending=False))
    
    # apply generated labels
    summary_df['label'] = summary_df.apply(lambda x: label_dict[x[category_col]], axis = 1)
    
    return summary_df
    
    cluster_summary = apply_and_summarize_labels(res, 'label_st1')

labeled_clusters = pd.merge(res, cluster_summary[['label_st1', 'label']], on='label_st1', how = 'left')
labeled_clusters[['text', 'category', 'label']].head(20)

labeled_clusters.head(20)

# pivot rows to more columns
label_pivot = labeled_clusters[~labeled_clusters.category.isnull()].drop_duplicates()
label_pivot = label_pivot[label_pivot.label_st1 != -1]
label_pivot['Yes'] = 1

label_pivot = pd.pivot_table(label_pivot, values='Yes', index=['category'], columns=['label'], aggfunc=np.sum)
label_pivot = label_pivot.reset_index()
label_pivot = label_pivot.sort_values([c for c in label_pivot.columns if c != 'category'], ascending = [False]*(len(label_pivot.columns)-1))
label_pivot.to_clipboard(sep = '|', index = False)
